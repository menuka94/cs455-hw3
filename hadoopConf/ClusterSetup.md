## Nodes
* Namenode: richmond
    * `springfield:32453`
* Secondary Namenode: richmond
    * `sacramento:32458`
* Yarn Resource Manager: atlanta
    * `springfield:32460`
 

## Workers

saint-paul
salem
salt-lake-city

santa-fe
albany
arkansas
madison
montgomery
columbus
helena
hartford
phoenix
sacramento
austin
raleigh
providence
nashville

### Commands
#### HDFS 
$HADOOP_HOME/bin/hdfs namenode -format

$HADOOP_HOME/sbin/start-dfs.sh

$HADOOP_HOME/sbin/stop-dfs.sh

$HADOOP_HOME/sbin/stop-all.sh

#### YARN
$HADOOP_HOME/sbin/start-yarn.sh
 
$HADOOP_HOME/sbin/stop-yarn.sh


## Accessing Shared Datasets
* Using ViewFS-based federation for sharing datasets due to space constraints
* HW3-PC MapReduce programs will deal with two namenodes
    * NameNode of your local HDFS cluster
        * Used for writing output generated by your programs
    * NameNode of the shared HDFS cluster
        * Hosts input data and made read-only 
        
 
### Port Forwading to access Web UIs
* `ssh -NL 10000:localhost:32453 menukaw@richmond.cs.colostate.edu`

[Hadoop: Setting up a Single Node Cluster](https://hadoop.apache.org/docs/stable/hadoop-project-dist
/hadoop-common
/SingleCluster.html)

